{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtsI9z8r74lv"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lucamaiano/reprodl/blob/main/food_classifier.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT66vo1E74lv"
      },
      "source": [
        "## 0. Importing PyTorch and setting up device-agnostic code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "64TjFuww74lv",
        "outputId": "49b0f1b7-f9ad-4412-c00a-8c8eb2b49663"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Note: this notebook requires torch >= 1.10.0\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igGpcphL74lw"
      },
      "source": [
        "And now let's follow best practice and setup device-agnostic code.\n",
        "\n",
        "> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going `Runtime -> Run before`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1iNtCC5E74lw",
        "outputId": "b4e56094-551e-40f0-fc25-325bc6bd6f3c"
      },
      "outputs": [],
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiOWVmbv74lw"
      },
      "source": [
        "## 1. Get data\n",
        "\n",
        "First thing's first we need some data. And like any good cooking show, some data has already been prepared for us. We're going to start small. Because we're not looking to train the biggest model or use the biggest dataset yet. Machine learning is an iterative process, start small, get something working and increase when necessary. The data we're going to be using is a subset of the [Food101 dataset](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/). Food101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).\n",
        "\n",
        "Instead of 101 food classes though, we're going to start with 3: pizza, steak and sushi. And instead of 1,000 images per class, we're going to start with a random 10% (start small, increase when necessary).\n",
        "\n",
        "If you'd like to see where the data came from you see the following resources:\n",
        "* Original [Food101 dataset and paper website](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/).\n",
        "* [`torchvision.datasets.Food101`](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) - the version of the data I downloaded for this notebook.\n",
        "* [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) - a notebook I used to format the Food101 dataset to use for this notebook.\n",
        "* [`data/pizza_steak_sushi.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip) - the zip archive of pizza, steak and sushi images from Food101, created with the notebook linked above.\n",
        "\n",
        "Let's write some code to download the formatted data from GitHub.\n",
        "\n",
        "> **Note:** The dataset we're about to use has been pre-formatted for what we'd like to use it for. However, you'll often have to format your own datasets for whatever problem you're working on. This is a regular practice in the machine learning world."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scGNBJwU74lx",
        "outputId": "bd5b8522-506d-4566-aa03-5fd44086d32f"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "        print(\"Downloading pizza, steak, sushi data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi data...\")\n",
        "        zip_ref.extractall(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MsCKHba74lx"
      },
      "source": [
        "## 2. Become one with the data (data preparation)\n",
        "\n",
        "Dataset downloaded! Time to become one with it. This is another important step before building a model. Before starting a project or building any kind of model, it's important to know what data you're working with.\n",
        "\n",
        "In our case, we have images of pizza, steak and sushi in standard image classification format. Image classification format contains separate classes of images in seperate directories titled with a particular class name. For example, all images of `pizza` are contained in the `pizza/` directory. This format is popular across many different image classification benchmarks, including [ImageNet](https://www.image-net.org/) (of the most popular computer vision benchmark datasets).\n",
        "\n",
        "You can see an example of the storage format below, the images numbers are arbitrary.\n",
        "\n",
        "```\n",
        "pizza_steak_sushi/ <- overall dataset folder\n",
        "    train/ <- training images\n",
        "        pizza/ <- class name as folder name\n",
        "            image01.jpeg\n",
        "            image02.jpeg\n",
        "            ...\n",
        "        steak/\n",
        "            image24.jpeg\n",
        "            image25.jpeg\n",
        "            ...\n",
        "        sushi/\n",
        "            image37.jpeg\n",
        "            ...\n",
        "    test/ <- testing images\n",
        "        pizza/\n",
        "            image101.jpeg\n",
        "            image102.jpeg\n",
        "            ...\n",
        "        steak/\n",
        "            image154.jpeg\n",
        "            image155.jpeg\n",
        "            ...\n",
        "        sushi/\n",
        "            image167.jpeg\n",
        "            ...\n",
        "```\n",
        "\n",
        "The goal will be to **take this data storage structure and turn it into a dataset usable with PyTorch**. We can inspect what's in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.\n",
        "\n",
        "To do so, we'll use Python's in-built [`os.walk()`](https://docs.python.org/3/library/os.html#os.walk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUOQ_MXQ74lx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "  Args:\n",
        "    dir_path (str or pathlib.Path): target directory\n",
        "\n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH54IIDK74lx",
        "outputId": "cd483004-517b-4346-cb08-4700ffa73395"
      },
      "outputs": [],
      "source": [
        "walk_through_dir(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTfR2AzD74lx"
      },
      "source": [
        "Excellent! It looks like we've got about 75 images per training class and 25 images per testing class. That should be enough to get started.\n",
        "While we're at it, let's setup our training and testing paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATM8Hns874lx",
        "outputId": "952f1888-544a-4aa4-8450-3b8af3ae8f13"
      },
      "outputs": [],
      "source": [
        "# Setup train and testing paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzFpPlhG74lx"
      },
      "source": [
        "### 2.1 Visualize an image\n",
        "\n",
        "Okay, we've seen how our directory structure is formatted.\n",
        "\n",
        "Now in the spirit of the data explorer, it's time to *visualize, visualize, visualize!*\n",
        "\n",
        "Let's write some code to:\n",
        "1. Get all of the image paths using [`pathlib.Path.glob()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.glob) to find all of the files ending in `.jpg`.\n",
        "2. Pick a random image path using Python's [`random.choice()`](https://docs.python.org/3/library/random.html#random.choice).\n",
        "3. Get the image class name using [`pathlib.Path.parent.stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.parent).\n",
        "4. And since we're working with images, we'll open the random image path using [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open) (PIL stands for Python Image Library).\n",
        "5. We will plot the image with [`matplotlib.pyplot.imshow()`](https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.imshow.html), but we have to convert it to a NumPy array first.\n",
        "6. We'll then show the image and print some metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "aQUrej9l74lx",
        "outputId": "55d51edc-e01d-42c2-be33-ca89353a1ce4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Set seed\n",
        "random.seed(42) # <- try changing this and see what happens\n",
        "\n",
        "# 1. Get all image paths (* means \"any combination\")\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "\n",
        "# 2. Get random image path\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
        "image_class = random_image_path.parent.stem\n",
        "\n",
        "# 4. Open image\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "# 5. Turn the image into an array\n",
        "img_as_array = np.asarray(img)\n",
        "\n",
        "# 6. Plot the image with matplotlib\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(img_as_array)\n",
        "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels]\")\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYDrKz3v74lx"
      },
      "source": [
        "## 3. Transforming data\n",
        "\n",
        "Now what if we wanted to load our image data into PyTorch?\n",
        "\n",
        "Before we can use our image data with PyTorch we need to:\n",
        "\n",
        "1. Turn it into tensors (numerical representations of our images).\n",
        "2. Turn it into a `torch.utils.data.Dataset` and subsequently a `torch.utils.data.DataLoader`, we'll call these `Dataset` and `DataLoader` for short.\n",
        "\n",
        "There are several different kinds of pre-built datasets and dataset loaders for PyTorch, depending on the problem you're working on.\n",
        "\n",
        "| **Problem space** | **Pre-built Datasets and Functions** |\n",
        "| ----- | ----- |\n",
        "| **Vision** | [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) |\n",
        "| **Audio** | [`torchaudio.datasets`](https://pytorch.org/audio/stable/datasets.html) |\n",
        "| **Text** | [`torchtext.datasets`](https://pytorch.org/text/stable/datasets.html) |\n",
        "| **Recommendation system** | [`torchrec.datasets`](https://pytorch.org/torchrec/torchrec.datasets.html) |\n",
        "\n",
        "Since we're working with a vision problem, we'll be looking at `torchvision.datasets` for our data loading functions as well as [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) for preparing our data.\n",
        "\n",
        "Let's import some base libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkfXqrYq74lx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TH89wvf74lx",
        "tags": []
      },
      "source": [
        "### 3.1 Transforming data with `torchvision.transforms`\n",
        "\n",
        "We've got folders of images but before we can use them with PyTorch, we need to convert them into tensors. One of the ways we can do this is by using the `torchvision.transforms` module.\n",
        "\n",
        "`torchvision.transforms` contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for **data augmentation** purposes.\n",
        "\n",
        "To get experience with `torchvision.transforms`, let's write a series of transform steps that:\n",
        "1. Resize the images using [`transforms.Resize()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize) (from about 512x512 to 64x64, the same shape as the images on the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/)).\n",
        "2. Flip our images randomly on the horizontal using [`transforms.RandomHorizontalFlip()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip) (this could be considered a form of data augmentation because it will artificially change our image data).\n",
        "3. Turn our images from a PIL image to a PyTorch tensor using [`transforms.ToTensor()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor).\n",
        "\n",
        "We can compile all of these steps using [`torchvision.transforms.Compose()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GG4cUbPM74lx"
      },
      "outputs": [],
      "source": [
        "# Write transform for image\n",
        "data_transform = transforms.Compose([\n",
        "    # Resize the images to 64x64\n",
        "    transforms.Resize(size=(64, 64)),\n",
        "    # Flip the images randomly on the horizontal\n",
        "    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
        "    # Turn the image into a torch.Tensor\n",
        "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dx08WAK74ly"
      },
      "source": [
        "Now we've got a composition of transforms, let's write a function to try them out on various images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AS1Yj8Y074ly",
        "outputId": "2cc0f734-d415-4014-ff4d-ae4d7d2e4333"
      },
      "outputs": [],
      "source": [
        "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
        "    \"\"\"Plots a series of random images from image_paths.\n",
        "\n",
        "    Will open n image paths from image_paths, transform them\n",
        "    with transform and plot them side by side.\n",
        "\n",
        "    Args:\n",
        "        image_paths (list): List of target image paths.\n",
        "        transform (PyTorch Transforms): Transforms to apply to images.\n",
        "        n (int, optional): Number of images to plot. Defaults to 3.\n",
        "        seed (int, optional): Random seed for the random generator. Defaults to 42.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    random_image_paths = random.sample(image_paths, k=n)\n",
        "    for image_path in random_image_paths:\n",
        "        with Image.open(image_path) as f:\n",
        "            fig, ax = plt.subplots(1, 2)\n",
        "            ax[0].imshow(f)\n",
        "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
        "            ax[0].axis(\"off\")\n",
        "\n",
        "            # Transform and plot image\n",
        "            # Note: permute() will change shape of image to suit matplotlib\n",
        "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
        "            transformed_image = transform(f).permute(1, 2, 0)\n",
        "            ax[1].imshow(transformed_image)\n",
        "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
        "            ax[1].axis(\"off\")\n",
        "\n",
        "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
        "\n",
        "plot_transformed_images(image_path_list,\n",
        "                        transform=data_transform,\n",
        "                        n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nEIAyIs74ly"
      },
      "source": [
        "Nice!\n",
        "\n",
        "We've now got a way to convert our images to tensors using `torchvision.transforms`.\n",
        "\n",
        "We also manipulate their size and orientation if needed (some models prefer images of different sizes and shapes).\n",
        "\n",
        "Generally, the larger the shape of the image, the more information a model can recover.\n",
        "\n",
        "For example, an image of size `[256, 256, 3]` will have 16x more pixels than an image of size `[64, 64, 3]` (`(256*256*3)/(64*64*3)=16`).\n",
        "\n",
        "However, the tradeoff is that more pixels requires more computations.\n",
        "\n",
        "> **Exercise:** Try commenting out one of the transforms in `data_transform` and running the plotting function `plot_transformed_images()` again, what happens?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvPKfolY74l2"
      },
      "source": [
        "## 4. Loading Image Data with a Custom `Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh7hdYCE74l2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from typing import Tuple, Dict, List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loO7rl6c74l2"
      },
      "source": [
        "### 4.1 Creating a helper function to get class names\n",
        "\n",
        "Let's write a helper function capable of creating a list of class names and a dictionary of class names and their indexes given a directory path.\n",
        "\n",
        "To do so, we'll:\n",
        "1. Get the class names using `os.scandir()` to traverse a target directory (ideally the directory is in standard image classification format).\n",
        "2. Raise an error if the class names aren't found (if this happens, there might be something wrong with the directory structure).\n",
        "3. Turn the class names into a dictionary of numerical labels, one for each class.\n",
        "\n",
        "Let's see a small example of step 1 before we write the full function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bAwU6tn74l2",
        "outputId": "945c816d-c0f0-40ec-a638-7eb6116069a8"
      },
      "outputs": [],
      "source": [
        "# Setup path for target directory\n",
        "target_directory = train_dir\n",
        "print(f\"Target directory: {target_directory}\")\n",
        "\n",
        "# Get the class names from the target directory\n",
        "class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\n",
        "print(f\"Class names found: {class_names_found}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IL_sW4Q74l2"
      },
      "source": [
        "Excellent!\n",
        "\n",
        "How about we turn it into a full function?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7PXg8B974l2"
      },
      "outputs": [],
      "source": [
        "# Make function to find classes in target directory\n",
        "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "    \"\"\"Finds the class folder names in a target directory.\n",
        "\n",
        "    Assumes target directory is in standard image classification format.\n",
        "\n",
        "    Args:\n",
        "        directory (str): target directory to load classnames from.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
        "\n",
        "    Example:\n",
        "        find_classes(\"food_images/train\")\n",
        "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
        "    \"\"\"\n",
        "    # 1. Get the class names by scanning the target directory\n",
        "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
        "\n",
        "    # 2. Raise an error if class names not found\n",
        "    if not classes:\n",
        "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
        "\n",
        "    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
        "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "    return classes, class_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Jdxc5v74l2"
      },
      "source": [
        "Looking good!\n",
        "\n",
        "Now let's test out our `find_classes()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD5LOhxG74l2",
        "outputId": "76d71d59-48b1-498e-8542-9800dd9c6d0b"
      },
      "outputs": [],
      "source": [
        "find_classes(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W4ed-zU74l2"
      },
      "source": [
        "Woohoo! Looking good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wavo8x3B74l2"
      },
      "source": [
        "### 4.2 Create a custom `Dataset` to replicate `ImageFolder`\n",
        "\n",
        "Now we're ready to build our own custom `Dataset`.\n",
        "\n",
        "We'll build one to replicate the functionality of `torchvision.datasets.ImageFolder()`.\n",
        "\n",
        "This will be good practice, plus, it'll reveal a few of the required steps to make your own custom `Dataset`.\n",
        "\n",
        "It'll be a fair bit of a code... but nothing we can't handle!\n",
        "\n",
        "Let's break it down:\n",
        "1. Subclass `torch.utils.data.Dataset`.\n",
        "2. Initialize our subclass with a `targ_dir` parameter (the target data directory) and `transform` parameter (so we have the option to transform our data if needed).\n",
        "3. Create several attributes for `paths` (the paths of our target images), `transform` (the transforms we might like to use, this can be `None`), `classes` and `class_to_idx` (from our `find_classes()` function).\n",
        "4. Create a function to load images from file and return them, this could be using `PIL` or [`torchvision.io`](https://pytorch.org/vision/stable/io.html#image) (for input/output of vision data).\n",
        "5. Overwrite the `__len__` method of `torch.utils.data.Dataset` to return the number of samples in the `Dataset`, this is recommended but not required. This is so you can call `len(Dataset)`.\n",
        "6. Overwrite the `__getitem__` method of `torch.utils.data.Dataset` to return a single sample from the `Dataset`, this is required.\n",
        "\n",
        "Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqru0mM_74l2"
      },
      "outputs": [],
      "source": [
        "# Write a custom dataset class (inherits from torch.utils.data.Dataset)\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 1. Subclass torch.utils.data.Dataset\n",
        "class ImageFolderCustom(Dataset):\n",
        "\n",
        "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
        "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
        "\n",
        "        # 3. Create class attributes\n",
        "        # Get all image paths\n",
        "        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
        "        # Setup transforms\n",
        "        self.transform = transform\n",
        "        # Create classes and class_to_idx attributes\n",
        "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
        "\n",
        "    # 4. Make function to load images\n",
        "    def load_image(self, index: int) -> Image.Image:\n",
        "        \"Opens an image via a path and returns it.\"\n",
        "        image_path = self.paths[index]\n",
        "        return Image.open(image_path)\n",
        "\n",
        "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
        "    def __len__(self) -> int:\n",
        "        \"Returns the total number of samples.\"\n",
        "        return len(self.paths)\n",
        "\n",
        "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
        "        \"Returns one sample of data, data and label (X, y).\"\n",
        "        img = self.load_image(index)\n",
        "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
        "        class_idx = self.class_to_idx[class_name]\n",
        "\n",
        "        # Transform if necessary\n",
        "        if self.transform:\n",
        "            return self.transform(img), class_idx # return data, label (X, y)\n",
        "        else:\n",
        "            return img, class_idx # return data, label (X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVPGQIkp74l2"
      },
      "source": [
        "Woah! A whole bunch of code to load in our images.\n",
        "\n",
        "This is one of the downsides of creating your own custom `Dataset`'s.\n",
        "\n",
        "However, now we've written it once, we could move it into a `.py` file such as `data_loader.py` along with some other helpful data functions and reuse it later on.\n",
        "\n",
        "Before we test out our new `ImageFolderCustom` class, let's create some transforms to prepare our images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04bdAO8474l2"
      },
      "outputs": [],
      "source": [
        "# Augment train data\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Don't augment test data, only reshape\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1b0y9C174l2"
      },
      "source": [
        "Now comes the moment of truth!\n",
        "\n",
        "Let's turn our training images (contained in `train_dir`) and our testing images (contained in `test_dir`) into `Dataset`'s using our own `ImageFolderCustom` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVUZZBZP74l2",
        "outputId": "04d8cf9d-e6e0-49c1-ece0-61ee9145c475"
      },
      "outputs": [],
      "source": [
        "train_data = ImageFolderCustom(targ_dir=train_dir,\n",
        "                                      transform=train_transforms)\n",
        "test_data = ImageFolderCustom(targ_dir=test_dir,\n",
        "                                     transform=test_transforms)\n",
        "train_data, test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C98SZ1dC74l2"
      },
      "source": [
        "Hmm... no errors, did it work?\n",
        "\n",
        "Let's try calling `len()` on our new `Dataset`'s and find the `classes` and `class_to_idx` attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl_0fVrN74l2",
        "outputId": "21df17d6-be69-4418-a365-cfdf305f66fa"
      },
      "outputs": [],
      "source": [
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li-sQOQK74l3",
        "outputId": "0f04c4c2-d3b8-4b46-9d15-c0acc6dfdbc3"
      },
      "outputs": [],
      "source": [
        "train_data.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuvfbTw074l3",
        "outputId": "c2068ffa-4c1d-4695-cee8-fa8fb1562d95"
      },
      "outputs": [],
      "source": [
        "train_data.class_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5mptKu74l3"
      },
      "source": [
        "### 4.3 Create a function to display random images\n",
        "\n",
        "You know what time it is!\n",
        "\n",
        "Time to put on our data explorer's hat and *visualize, visualize, visualize!*\n",
        "\n",
        "Let's create a helper function called `display_random_images()` that helps us visualize images in our `Dataset'`s.\n",
        "\n",
        "Specifically, it'll:\n",
        "1. Take in a `Dataset` and a number of other parameters such as `classes` (the names of our target classes), the number of images to display (`n`) and a random seed.\n",
        "2. To prevent the display getting out of hand, we'll cap `n` at 10 images.\n",
        "3. Set the random seed for reproducible plots (if `seed` is set).\n",
        "4. Get a list of random sample indexes (we can use Python's `random.sample()` for this) to plot.\n",
        "5. Setup a `matplotlib` plot.\n",
        "6. Loop through the random sample indexes found in step 4 and plot them with `matplotlib`.\n",
        "7. Make sure the sample images are of shape `HWC` (height, width, color channels) so we can plot them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F-cjxvq74l3"
      },
      "outputs": [],
      "source": [
        "# 1. Take in a Dataset as well as a list of class names\n",
        "def display_random_images(dataset: torch.utils.data.dataset.Dataset,\n",
        "                          classes: List[str] = None,\n",
        "                          n: int = 10,\n",
        "                          display_shape: bool = True,\n",
        "                          seed: int = None):\n",
        "\n",
        "    # 2. Adjust display if n too high\n",
        "    if n > 10:\n",
        "        n = 10\n",
        "        display_shape = False\n",
        "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
        "\n",
        "    # 3. Set random seed\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "\n",
        "    # 4. Get random sample indexes\n",
        "    random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
        "\n",
        "    # 5. Setup plot\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    # 6. Loop through samples and display random samples\n",
        "    for i, targ_sample in enumerate(random_samples_idx):\n",
        "        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
        "\n",
        "        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
        "        targ_image_adjust = targ_image.permute(1, 2, 0)\n",
        "\n",
        "        # Plot adjusted samples\n",
        "        plt.subplot(1, n, i+1)\n",
        "        plt.imshow(targ_image_adjust)\n",
        "        plt.axis(\"off\")\n",
        "        if classes:\n",
        "            title = f\"class: {classes[targ_label]}\"\n",
        "            if display_shape:\n",
        "                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n",
        "        plt.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgFWJD5974l3"
      },
      "source": [
        "What a good looking function!\n",
        "\n",
        "Let's test it out first with the `Dataset` we created with `torchvision.datasets.ImageFolder()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7khUrTOeDQiw",
        "outputId": "d5bf6370-1c3e-4458-d545-9073b5a73646"
      },
      "outputs": [],
      "source": [
        "# Get class names as a list\n",
        "class_names = train_data.classes\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Nh6_8LOQ74l3",
        "outputId": "7ccdd67f-66f2-4603-83cf-ef935a2dc5d7"
      },
      "outputs": [],
      "source": [
        "# Display random images from ImageFolder created Dataset\n",
        "display_random_images(train_data,\n",
        "                      n=5,\n",
        "                      classes=class_names,\n",
        "                      seed=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2MDCUbe74l3"
      },
      "source": [
        "### 4.4 Turn custom loaded images into `DataLoader`'s\n",
        "\n",
        "We've got a way to turn our raw images into `Dataset`'s (features mapped to labels or `X`'s mapped to `y`'s) through our `ImageFolderCustom` class.\n",
        "\n",
        "Now how could we turn our custom `Dataset`'s into `DataLoader`'s?\n",
        "\n",
        "If you guessed by using `torch.utils.data.DataLoader()`, you'd be right!\n",
        "\n",
        "Because our custom `Dataset`'s subclass `torch.utils.data.Dataset`, we can use them directly with `torch.utils.data.DataLoader()`.\n",
        "\n",
        "And we can do using very similar steps to before except this time we'll be using our custom created `Dataset`'s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0HNIbym74l3",
        "outputId": "c511a9af-5abf-4fba-d65a-7332e8d95198"
      },
      "outputs": [],
      "source": [
        "# Turn train and test custom Dataset's into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(dataset=train_data, # use custom created train Dataset\n",
        "                                     batch_size=1, # how many samples per batch?\n",
        "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data, # use custom created test Dataset\n",
        "                                    batch_size=1,\n",
        "                                    num_workers=0,\n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku0rhehL74l3"
      },
      "source": [
        "Do the shapes of the samples look the same?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC9Bv-ZF74l3",
        "outputId": "2e256f49-d919-4d72-e7af-a085325bbf37"
      },
      "outputs": [],
      "source": [
        "# Get image and label from custom DataLoader\n",
        "img_custom, label_custom = next(iter(train_dataloader))\n",
        "\n",
        "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
        "print(f\"Image shape: {img_custom.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Label shape: {label_custom.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAxvY38H74l3"
      },
      "source": [
        "They sure do!\n",
        "\n",
        "Let's now take a lot at some other forms of data transforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KABdRRxE74l4"
      },
      "source": [
        "## 5. Model 0: TinyVGG without data augmentation\n",
        "\n",
        "Alright, we've seen how to turn our data from images in folders to transformed tensors.\n",
        "\n",
        "Now let's construct a computer vision model to see if we can classify if an image is of pizza, steak or sushi.\n",
        "\n",
        "To begin, we'll start with a simple transform, only resizing the images to `(64, 64)` and turning them into tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tA2E0c74l4"
      },
      "source": [
        "### 5.1 Creating transforms and loading data for Model 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GJSfadP74l4"
      },
      "outputs": [],
      "source": [
        "# Create simple transform\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44RXr65a74l4"
      },
      "source": [
        "Excellent, now we've got a simple transform, let's:\n",
        "1. Load the data, turning each of our training and test folders first into a `Dataset` with `torchvision.datasets.ImageFolder()`\n",
        "2. Then into a `DataLoader` using `torch.utils.data.DataLoader()`.\n",
        "    * We'll set the `batch_size=32` and `num_workers` to as many CPUs on our machine (this will depend on what machine you're using)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDXIE53c74l4",
        "outputId": "1f0fae5e-0855-4cc0-9740-83c3c594aae5"
      },
      "outputs": [],
      "source": [
        "# 1. Load and transform data\n",
        "from torchvision import datasets\n",
        "train_data_simple = ImageFolderCustom(targ_dir=train_dir, transform=simple_transform)\n",
        "test_data_simple = ImageFolderCustom(targ_dir=test_dir, transform=simple_transform)\n",
        "\n",
        "# 2. Turn data into DataLoaders\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup batch size and number of workers\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
        "\n",
        "# Create DataLoader's\n",
        "train_dataloader_simple = DataLoader(dataset=train_data, # use custom created train Dataset\n",
        "                                     batch_size=BATCH_SIZE, # how many samples per batch?\n",
        "                                     num_workers=NUM_WORKERS, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "test_dataloader_simple = DataLoader(dataset=test_data, # use custom created test Dataset\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    num_workers=NUM_WORKERS,\n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_dataloader_simple, test_dataloader_simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH9jItRn74l4"
      },
      "source": [
        "`DataLoader`'s created!\n",
        "\n",
        "Let's build a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cduo92g74l4"
      },
      "source": [
        "### 5.2 Create TinyVGG model class\n",
        "\n",
        "In [notebook 03](https://www.learnpytorch.io/03_pytorch_computer_vision/#7-model-2-building-a-convolutional-neural-network-cnn), we used the TinyVGG model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "Let's recreate the same model, except this time we'll be using color images instead of grayscale (`in_channels=3` instead of `in_channels=1` for RGB pixels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWPpSx1r74l4",
        "outputId": "75c62408-6ba6-4309-8673-4265aa8a7668"
      },
      "outputs": [],
      "source": [
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "            nn.Linear(in_features=hidden_units*16*16,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.conv_block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "model_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNfEzI8V74l4"
      },
      "source": [
        "> **Note:** One of the ways to speed up deep learning models computing on a GPU is to leverage **operator fusion**.\n",
        ">\n",
        "> This means in the `forward()` method in our model above, instead of calling a layer block and reassigning `x` every time, we call each block in succession (see the final line of the `forward()` method in the model above for an example).\n",
        ">\n",
        "> This saves the time spent reassigning `x` (memory heavy) and focuses on only computing on `x`.\n",
        ">\n",
        "> See [*Making Deep Learning Go Brrrr From First Principles*](https://horace.io/brrr_intro.html) by Horace He for more ways on how to speed up machine learning models.\n",
        "\n",
        "Now that's a nice looking model!\n",
        "\n",
        "How about we test it out with a forward pass on a single image?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbS-Ry5h74l4"
      },
      "source": [
        "### 5.3 Try a forward pass on a single image (to test the model)\n",
        "\n",
        "A good way to test a model is to do a forward pass on a single piece of data.\n",
        "\n",
        "It's also handy way to test the input and output shapes of our different layers.\n",
        "\n",
        "To do a forward pass on a single image, let's:\n",
        "1. Get a batch of images and labels from the `DataLoader`.\n",
        "2. Get a single image from the batch and `unsqueeze()` the image so it has a batch size of `1` (so its shape fits the model).\n",
        "3. Perform inference on a single image (making sure to send the image to the target `device`).\n",
        "4. Print out what's happening and convert the model's raw output logits to prediction probabilities with `torch.softmax()` (since we're working with multi-class data) and convert the prediction probabilities to prediction labels with `torch.argmax()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kM0DRy474l4",
        "outputId": "9afad050-7804-476d-a1c0-3bd8602bea8d"
      },
      "outputs": [],
      "source": [
        "# 1. Get a batch of images and labels from the DataLoader\n",
        "img_batch, label_batch = next(iter(train_dataloader_simple))\n",
        "\n",
        "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
        "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
        "print(f\"Single image shape: {img_single.shape}\\n\")\n",
        "\n",
        "# 3. Perform a forward pass on a single image\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "    pred = model_0(img_single.to(device))\n",
        "\n",
        "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
        "print(f\"Output logits:\\n{pred}\\n\")\n",
        "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
        "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
        "print(f\"Actual label:\\n{label_single}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV_7Rb8o74l4"
      },
      "source": [
        "Wonderful, it looks like our model is outputting what we'd expect it to output.\n",
        "\n",
        "You can run the cell above a few times and each time have a different image be predicted on.\n",
        "\n",
        "And you'll probably notice the predictions are often wrong.\n",
        "\n",
        "This is to be expected because the model hasn't been trained yet and it's essentially guessing using random weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbKHQf7P74l4",
        "tags": []
      },
      "source": [
        "### 5.4 Use `torchinfo` to get an idea of the shapes going through our model\n",
        "\n",
        "Printing out our model with `print(model)` gives us an idea of what's going on with our model.\n",
        "\n",
        "And we can print out the shapes of our data throughout the `forward()` method.\n",
        "\n",
        "However, a helpful way to get information from our model is to use [`torchinfo`](https://github.com/TylerYep/torchinfo).\n",
        "\n",
        "`torchinfo` comes with a `summary()` method that takes a PyTorch model as well as an `input_shape` and returns what happens as a tensor moves through your model.\n",
        "\n",
        "> **Note:** If you're using Google Colab, you'll need to install `torchinfo`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRT1Tvi-74l4",
        "outputId": "00edd7ee-08ce-476a-c09d-5916e040139c"
      },
      "outputs": [],
      "source": [
        "# Install torchinfo if it's not available, import it if it is\n",
        "try:\n",
        "    import torchinfo\n",
        "except:\n",
        "    !pip install torchinfo\n",
        "    import torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "summary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVlFUr_e74l4"
      },
      "source": [
        "Nice! The output of `torchinfo.summary()` gives us a whole bunch of information about our model such as `Total params`, the total number of parameters in our model, or the `Estimated Total Size (MB)` which is the size of our model.\n",
        "You can also see the change in input and output shapes as data of a certain `input_size` moves through our model.\n",
        "\n",
        "Right now, our parameter numbers and total model size is low. This because we're starting with a small model, and if we need to increase its size later, we can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh-OMIsC74l4"
      },
      "source": [
        "### 5.5 Create train & test loop functions\n",
        "\n",
        "We've got data and we've got a model. Now let's make some training and test loop functions to train our model on the training data and evaluate our model on the testing data and to make sure we can use these the training and testing loops again, we'll functionize them.\n",
        "\n",
        "Specifically, we're going to make three functions:\n",
        "1. `train_step()` - takes in a model, a `DataLoader`, a loss function and an optimizer and trains the model on the `DataLoader`.\n",
        "2. `test_step()` - takes in a model, a `DataLoader` and a loss function and evaluates the model on the `DataLoader`.\n",
        "3. `train()` - performs 1. and 2. together for a given number of epochs and returns a results dictionary.\n",
        "\n",
        "Let's start by building `train_step()`.\n",
        "\n",
        "Because we're dealing with batches in the `DataLoader`'s, we'll accumulate the model loss and accuracy values during training (by adding them up for each batch) and then adjust them at the end before we return them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moD40fy874l4"
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    # Put model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # Setup train loss and train accuracy values\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    # Loop through data loader data batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Send data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate  and accumulate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metric across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmkhUjFf74l4"
      },
      "source": [
        "Woohoo! `train_step()` function done.\n",
        "\n",
        "Now let's do the same for the `test_step()` function.\n",
        "\n",
        "The main difference here will be the `test_step()` won't take in an optimizer and therefore won't perform gradient descent.\n",
        "\n",
        "But since we'll be doing inference, we'll make sure to turn on the `torch.inference_mode()` context manager for making predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmwpW4iH74l4"
      },
      "outputs": [],
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module):\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Setup test loss and test accuracy values\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        # Loop through DataLoader batches\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Send data to target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # 2. Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTnSTuHw74l4"
      },
      "source": [
        "Excellent!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nx5Yi2g74l4"
      },
      "source": [
        "### 5.6 Creating a `train()` function to combine `train_step()` and `test_step()`\n",
        "\n",
        "Now we need a way to put our `train_step()` and `test_step()` functions together.\n",
        "\n",
        "To do so, we'll package them up in a `train()` function.\n",
        "\n",
        "This function will train the model as well as evaluate it.\n",
        "\n",
        "Specificially, it'll:\n",
        "1. Take in a model, a `DataLoader` for training and test sets, an optimizer, a loss function and how many epochs to perform each train and test step for.\n",
        "2. Create an empty results dictionary for `train_loss`, `train_acc`, `test_loss` and `test_acc` values (we can fill this up as training goes on).\n",
        "3. Loop through the training and test step functions for a number of epochs.\n",
        "4. Print out what's happening at the end of each epoch.\n",
        "5. Update the empty results dictionary with the updated metrics each epoch.\n",
        "6. Return the filled\n",
        "\n",
        "To keep track of the number of epochs we've been through, let's import `tqdm` from `tqdm.auto` ([`tqdm`](https://github.com/tqdm/tqdm) is one of the most popular progress bar libraries for Python and `tqdm.auto` automatically decides what kind of progress bar is best for your computing environment, e.g. Jupyter Notebook vs. Python script)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV-TAMy374l5"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Take in various parameters required for training and test steps\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
        "          epochs: int = 5):\n",
        "\n",
        "    # 2. Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # 3. Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                           dataloader=train_dataloader,\n",
        "                                           loss_fn=loss_fn,\n",
        "                                           optimizer=optimizer)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn)\n",
        "\n",
        "        # 4. Print out what's happening\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # 5. Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # 6. Return the filled results at the end of the epochs\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PPmypF-74l5"
      },
      "source": [
        "### 5.7 Train and Evaluate Model 0\n",
        "\n",
        "Alright, alright, alright we've got all of the ingredients we need to train and evaluate our model.\n",
        "\n",
        "Time to put our `TinyVGG` model, `DataLoader`'s and `train()` function together to see if we can build a model capable of discerning between pizza, steak and sushi!\n",
        "\n",
        "Let's recreate `model_0` (we don't need to but we will for completeness) then call our `train()` function passing in the necessary parameters.\n",
        "\n",
        "To keep our experiments quick, we'll train our model for **5 epochs** (though you could increase this if you want).\n",
        "\n",
        "As for an **optimizer** and **loss function**, we'll use `torch.nn.CrossEntropyLoss()` (since we're working with multi-class classification data) and `torch.optim.Adam()` with a learning rate of `1e-3` respecitvely.\n",
        "\n",
        "To see how long things take, we'll import Python's [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer) method to calculate the training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "ba394d6b117a4b0aacc3e15990fefe8e",
            "3fda915bb2d640e082c5089ba0469a5e",
            "66e92651313f4afa9d30f2a1ebd09f20",
            "28e1956beb814a10b3cb34dae5c87f9e",
            "cff909918cc54513820f0a8a24dd57cd",
            "762479bb744547a39cf67136f286f78a",
            "bb7f7d862a7e4fd49a1c0b0fd6138ca4",
            "007710231a5449ff964e5f97db350d9c",
            "78f78732a62849ffad723b59e5fb2d29",
            "81585de2060445628b6b2f1ad23b311d",
            "5c82c78179b54a5d8b860e3e0e6cd9cf"
          ]
        },
        "id": "UOqycDcL74l5",
        "outputId": "15c44c22-dbfe-4542-8086-0f8af1c6b580"
      },
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Recreate an instance of TinyVGG\n",
        "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Train model_0\n",
        "model_0_results = train(model=model_0,\n",
        "                        train_dataloader=train_dataloader_simple,\n",
        "                        test_dataloader=test_dataloader_simple,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2tsjkr074l5"
      },
      "source": [
        "Hmm...\n",
        "\n",
        "It looks like our model performed pretty poorly.\n",
        "\n",
        "But that's okay for now, we'll keep persevering.\n",
        "\n",
        "What are some ways you could potentially improve it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixQEPvYR74l5"
      },
      "source": [
        "### 5.8 Plot the loss curves of Model 0\n",
        "\n",
        "From the print outs of our `model_0` training, it didn't look like it did too well.\n",
        "\n",
        "But we can further evaluate it by plotting the model's **loss curves**.\n",
        "\n",
        "**Loss curves** show the model's results over time.\n",
        "\n",
        "And they're a great way to see how your model performs on different datasets (e.g. training and test).\n",
        "\n",
        "Let's create a function to plot the values in our `model_0_results` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtWgMHBe74l5",
        "outputId": "858fb03f-8f28-4e63-f57c-43011fea1e7a"
      },
      "outputs": [],
      "source": [
        "# Check the model_0_results keys\n",
        "model_0_results.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcRVh9Ee74l5"
      },
      "source": [
        "We'll need to extract each of these keys and turn them into a plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Zg7O3_S74l5"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "    \"\"\"Plots training curves of a results dictionary.\n",
        "\n",
        "    Args:\n",
        "        results (dict): dictionary containing list of values, e.g.\n",
        "            {\"train_loss\": [...],\n",
        "             \"train_acc\": [...],\n",
        "             \"test_loss\": [...],\n",
        "             \"test_acc\": [...]}\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the loss values of the results dictionary (training and test)\n",
        "    loss = results['train_loss']\n",
        "    test_loss = results['test_loss']\n",
        "\n",
        "    # Get the accuracy values of the results dictionary (training and test)\n",
        "    accuracy = results['train_acc']\n",
        "    test_accuracy = results['test_acc']\n",
        "\n",
        "    # Figure out how many epochs there were\n",
        "    epochs = range(len(results['train_loss']))\n",
        "\n",
        "    # Setup a plot\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, label='train_loss')\n",
        "    plt.plot(epochs, test_loss, label='test_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
        "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7UegWMA74l5"
      },
      "source": [
        "Okay, let's test our `plot_loss_curves()` function out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "sOObSAHc74l5",
        "outputId": "6b3f2c8d-a936-42cb-fc3d-b38000439f31"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(model_0_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nqhx6wg74l5"
      },
      "source": [
        "Woah.\n",
        "\n",
        "Looks like things are all over the place...\n",
        "\n",
        "But we kind of knew that because our model's print out results during training didn't show much promise.\n",
        "\n",
        "You could try training the model for longer and see what happens when you plot a loss curve over a longer time horizon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfjOBnEz74l5"
      },
      "source": [
        "## 6. What should an ideal loss curve look like?\n",
        "\n",
        "Looking at training and test loss curves is a great way to see if your model is **overfitting**.\n",
        "\n",
        "An overfitting model is one that performs better (often by a considerable margin) on the training set than the validation/test set.\n",
        "\n",
        "If your training loss is far lower than your test loss, your model is **overfitting**.\n",
        "\n",
        "As in, it's learning the patterns in the training too well and those patterns aren't generalizing to the test data.\n",
        "\n",
        "The other side is when your training and test loss are not as low as you'd like, this is considered **underfitting**.\n",
        "\n",
        "The ideal position for a training and test loss curve is for them to line up closely with each other.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-loss-curves-overfitting-underfitting-ideal.jpg\" alt=\"different training and test loss curves illustrating overfitting, underfitting and the ideal loss curves\" width=\"800\"/>\n",
        "\n",
        "*Left: If your training and test loss curves aren't as low as you'd like, this is considered **underfitting**. *Middle:* When your test/validation loss is higher than your training loss this is considered **overfitting**. *Right:* The ideal scenario is when your training and test loss curves line up over time. This means your model is generalizing well. There are more combinations and different things loss curves can do, for more on these, see Google's [Interpreting Loss Curves guide](https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u71NAZ8-74l5"
      },
      "source": [
        "### 6.1 How to deal with overfitting\n",
        "\n",
        "Since the main problem with overfitting is that you're model is fitting the training data *too well*, you'll want to use techniques to \"reign it in\".\n",
        "\n",
        "A common technique of preventing overfitting is known as [**regularization**](https://ml-cheatsheet.readthedocs.io/en/latest/regularization.html).\n",
        "\n",
        "I like to think of this as \"making our models more regular\", as in, capable of fitting *more* kinds of data.\n",
        "\n",
        "Let's discuss a few methods to prevent overfitting.\n",
        "\n",
        "| **Method to prevent overfitting** | **What is it?** |\n",
        "| ----- | ----- |\n",
        "| **Get more data** | Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples. |\n",
        "| **Simplify your model** | If the current model is already overfitting the training data, it may be too complicated of a model. This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer. |\n",
        "| **Use data augmentation** | [**Data augmentation**](https://developers.google.com/machine-learning/glossary#data-augmentation) manipulates the training data in a way so that's harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data. |\n",
        "| **Use transfer learning** | [**Transfer learning**](https://developers.google.com/machine-learning/glossary#transfer-learning) involves leveraging the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images. |\n",
        "| **Use dropout layers** | Dropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See [`torch.nn.Dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) for more. |\n",
        "| **Use learning rate decay** | The idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to [**convergence**](https://developers.google.com/machine-learning/glossary#convergence), the smaller you'll want your weight updates to be.  |\n",
        "| **Use early stopping** | [**Early stopping**](https://developers.google.com/machine-learning/glossary#early_stopping) stops model training *before* it begins to overfit. As in, say the model's loss has stopped decreasing for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model weights that had the lowest loss (10 epochs prior). |\n",
        "\n",
        "There are more methods for dealing with overfitting but these are some of the main ones.\n",
        "\n",
        "As you start to build more and more deep models, you'll find because deep learnings are *so good* at learning patterns in data, dealing with overfitting is one of the primary problems of deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prJqKYnL74l5"
      },
      "source": [
        "### 6.2 How to deal with underfitting\n",
        "\n",
        "When a model is [**underfitting**](https://developers.google.com/machine-learning/glossary#underfitting) it is considered to have poor predictive power on the training and test sets.\n",
        "\n",
        "In essence, an underfitting model will fail to reduce the loss values to a desired level.\n",
        "\n",
        "Right now, looking at our current loss curves, I'd considered our `TinyVGG` model, `model_0`, to be underfitting the data.\n",
        "\n",
        "The main idea behind dealing with underfitting is to *increase* your model's predictive power.\n",
        "\n",
        "There are several ways to do this.\n",
        "\n",
        "| **Method to prevent underfitting** | **What is it?** |\n",
        "| ----- | ----- |\n",
        "| **Add more layers/units to your model** | If your model is underfitting, it may not have enough capability to *learn* the required patterns/weights/representations of the data to be predictive. One way to add more predictive power to your model is to increase the number of hidden layers/units within those layers. |\n",
        "| **Tweak the learning rate** | Perhaps your model's learning rate is too high to begin with. And it's trying to update its weights each epoch too much, in turn not learning anything. In this case, you might lower the learning rate and see what happens. |\n",
        "| **Use transfer learning** | Transfer learning is capable of preventing overfitting and underfitting. It involves using the patterns from a previously working model and adjusting them to your own problem. |\n",
        "| **Train for longer** | Sometimes a model just needs more time to learn representations of data. If you find in your smaller experiments your model isn't learning anything, perhaps leaving it train for a more epochs may result in better performance. |\n",
        "| **Use less regularization** | Perhaps your model is underfitting because you're trying to prevent overfitting too much. Holding back on regularization techniques can help your model fit the data better. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcX-vUWL74l5"
      },
      "source": [
        "### 6.3 The balance between overfitting and underfitting\n",
        "\n",
        "None of the methods discussed above are silver bullets, meaning, they don't always work.\n",
        "\n",
        "And preventing overfitting and underfitting is possibly the most active area of machine learning research.\n",
        "\n",
        "Since everone wants their models to fit better (less underfitting) but not so good they don't generalize well and perform in the real world (less overfitting).\n",
        "\n",
        "There's a fine line between overfitting and underfitting.\n",
        "\n",
        "Because too much of each can cause the other.\n",
        "\n",
        "Transfer learning is perhaps one of the most powerful techniques when it comes to dealing with both overfitting and underfitting on your own problems.\n",
        "\n",
        "Rather than handcraft different overfitting and underfitting techniques, transfer learning enables you to take an already working model in a similar problem space to yours (say one from [paperswithcode.com/sota](https://paperswithcode.com/sota) or [Hugging Face models](https://huggingface.co/models)) and apply it to your own dataset.\n",
        "\n",
        "We'll see the power of transfer learning in a later notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45T8IYDL74l5"
      },
      "source": [
        "## 7. Model 1: TinyVGG with Data Augmentation\n",
        "\n",
        "Machine learning is all about harnessing the power of randomness and research shows that random transforms (like [`transforms.RandAugment()`](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#randaugment) and [`transforms.TrivialAugmentWide()`](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#trivialaugmentwide)) generally perform better than hand-picked transforms. The idea behind [TrivialAugment](https://arxiv.org/abs/2103.10158) is... well, trivial. You have a set of transforms and you randomly pick a number of them to perform on an image and at a random magnitude between a given range (a higher magnitude means more instense).\n",
        "\n",
        "The main parameter to pay attention to in `transforms.TrivialAugmentWide()` is `num_magnitude_bins=31`. It defines how much of a range an intensity value will be picked to apply a certain transform, `0` being no range and `31` being maximum range (highest chance for highest intensity). \n",
        "\n",
        "Let's test our data augmentation out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all image paths\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "\n",
        "# Plot random images\n",
        "plot_transformed_images(\n",
        "    image_paths=image_path_list,\n",
        "    transform=train_transforms,\n",
        "    n=3,\n",
        "    seed=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Time to try out another model! This time, let's load in the data and use **data augmentation** to see if it improves our results in anyway.\n",
        "\n",
        "First, we'll compose a training transform to include `transforms.TrivialAugmentWide()` as well as resize and turn our images into tensors.\n",
        "\n",
        "We'll do the same for a testing transform except without the data augmentation.\n",
        "\n",
        "### 7.1 Create transform with data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuqZMcud74l5"
      },
      "outputs": [],
      "source": [
        "# Create training transform with TrivialAugment\n",
        "train_transform_trivial_augment = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create testing transform (no data augmentation)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjuMd7rH74l5"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "Now let's turn our images into `Dataset`'s using `torchvision.datasets.ImageFolder()` and then into `DataLoader`'s with `torch.utils.data.DataLoader()`.\n",
        "\n",
        "### 7.2 Create train and test `Dataset`'s and `DataLoader`'s\n",
        "\n",
        "We'll make sure the train `Dataset` uses the `train_transform_trivial_augment` and the test `Dataset` uses the `test_transform`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JDHvWEd74l5",
        "outputId": "13de5c6b-bbd8-454c-9fc0-881212b49974"
      },
      "outputs": [],
      "source": [
        "# Turn image folders into Datasets\n",
        "train_data_augmented = ImageFolderCustom(targ_dir=train_dir, transform=train_transform_trivial_augment)\n",
        "test_data_simple = ImageFolderCustom(targ_dir=test_dir, transform=test_transform)\n",
        "\n",
        "train_data_augmented, test_data_simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FKz_yzp74l5"
      },
      "source": [
        "And we'll make `DataLoader`'s with a `batch_size=32` and with `num_workers` set to the number of CPUs available on our machine (we can get this using Python's `os.cpu_count()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "42hwUumv74l5",
        "outputId": "c4745562-af87-4998-f38d-0e1348ed60df"
      },
      "outputs": [],
      "source": [
        "# Turn Datasets into DataLoader's\n",
        "import os\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "train_dataloader_augmented = DataLoader(train_data_augmented,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                        num_workers=NUM_WORKERS)\n",
        "\n",
        "test_dataloader_simple = DataLoader(test_data_simple,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    shuffle=False,\n",
        "                                    num_workers=NUM_WORKERS)\n",
        "\n",
        "train_dataloader_augmented, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWcTvKY874l6"
      },
      "source": [
        "### 7.3 Construct and train Model 1\n",
        "\n",
        "Data loaded!\n",
        "\n",
        "Now to build our next model, `model_1`, we can reuse our `TinyVGG` class from before.\n",
        "\n",
        "We'll make sure to send it to the target device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jEZhRuC74l6",
        "outputId": "968d679e-23d5-40e3-aa87-3aed6fd75ce0"
      },
      "outputs": [],
      "source": [
        "# Create model_1 and send it to the target device\n",
        "torch.manual_seed(42)\n",
        "model_1 = TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(train_data_augmented.classes)).to(device)\n",
        "model_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3eaet3874l6"
      },
      "source": [
        "Model ready!\n",
        "\n",
        "Time to train!\n",
        "\n",
        "Since we've already got functions for the training loop (`train_step()`) and testing loop (`test_step()`) and a function to put them together in `train()`, let's reuse those.\n",
        "\n",
        "We'll use the same setup as `model_0` with only the `train_dataloader` parameter varying:\n",
        "* Train for 5 epochs.\n",
        "* Use `train_dataloader=train_dataloader_augmented` as the training data in `train()`.\n",
        "* Use `torch.nn.CrossEntropyLoss()` as the loss function (since we're working with multi-class classification).\n",
        "* Use `torch.optim.Adam()` with `lr=0.001` as the learning rate as the optimizer.                                                          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "42ef049d73e64ea58e73773060710c57",
            "7491de13f92e4d5eafaa2d0b962475ef",
            "1bca8cd9332342bca58ad5166b7846d2",
            "36166176a93444debf0869de3998c5b8",
            "39bf1e9ad56048f7ac25477f0331b5cb",
            "2bd81d26df9943db88d802b4d8d90ad9",
            "4e4165a0ef324ebaa95195c924dd7ef4",
            "ea57a3613d3d40e09344b97cf2d86150",
            "0ff6fe61a1a74ca5a499cf7217a4bfed",
            "3dc74616c7884742a6065b2124e2ca53",
            "ebf44251016840d9a8cad51dff0c9900"
          ]
        },
        "id": "s30K6pEZ74l7",
        "outputId": "f78a0d95-1bef-48be-d7a6-36e9f5647c4a"
      },
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Train model_1\n",
        "model_1_results = train(model=model_1,\n",
        "                        train_dataloader=train_dataloader_augmented,\n",
        "                        test_dataloader=test_dataloader_simple,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU-2yx1E74l7"
      },
      "source": [
        "Hmm...\n",
        "\n",
        "It doesn't look like our model performed very well again.\n",
        "\n",
        "Let's check out its loss curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prpuv64-74l7"
      },
      "source": [
        "### 7.4 Plot the loss curves of Model 1\n",
        "\n",
        "Since we've got the results of `model_1` saved in a results dictionary, `model_1_results`, we can plot them using `plot_loss_curves()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "kHYt8Tbc74l7",
        "outputId": "ba0e74eb-24be-469c-f35f-9054e0062fe8"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(model_1_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIYW1Lu074l7"
      },
      "source": [
        "Wow...\n",
        "\n",
        "These don't look very good either...\n",
        "\n",
        "Is our model **underfitting** or **overfitting**?\n",
        "\n",
        "Or both?\n",
        "\n",
        "Ideally we'd like it have higher accuracy and lower loss right?\n",
        "\n",
        "What are some methods you could try to use to achieve these?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwxeMRB74l7"
      },
      "source": [
        "## 8. Compare model results\n",
        "\n",
        "Even though our models our performing quite poorly, we can still write code to compare them.\n",
        "\n",
        "Let's first turn our model results in pandas DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tr10SSiS74l7",
        "outputId": "907f0975-5fbb-4aab-bdbd-10a5695934e6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "model_0_df = pd.DataFrame(model_0_results)\n",
        "model_1_df = pd.DataFrame(model_1_results)\n",
        "model_0_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hNvkadV74l7"
      },
      "source": [
        "And now we can write some plotting code using `matplotlib` to visualize the results of `model_0` and `model_1` together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "uzLQZUY874l7",
        "outputId": "b2f11cce-a928-4f89-835d-ddb47dfb847e"
      },
      "outputs": [],
      "source": [
        "# Setup a plot\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(model_0_df))\n",
        "\n",
        "# Plot train loss\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot test loss\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\n",
        "plt.title(\"Test Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot train accuracy\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\n",
        "plt.title(\"Train Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot test accuracy\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\n",
        "plt.title(\"Test Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVtcu2nf74l7"
      },
      "source": [
        "It looks like our models both performed equally poorly and were kind of sporadic (the metrics go up and down sharply).\n",
        "\n",
        "If you built `model_2`, what would you do differently to try and improve performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96C5mcu374l7",
        "tags": []
      },
      "source": [
        "## 9. Make a prediction on a custom image\n",
        "\n",
        "If you've trained a model on a certain dataset, chances are you'd like to make a prediction on on your own custom data.\n",
        "\n",
        "In our case, since we've trained a model on pizza, steak and sushi images, how could we use our model to make a prediction on one of our own images? To do so, we can load an image and then **preprocess it in a way that matches the type of data our model was trained on**. In other words, we'll have to convert our own custom image to a tensor and make sure it's in the right datatype before passing it to our model.\n",
        "\n",
        "Let's start by downloading a custom image. Since our model predicts whether an image contains pizza, steak or sushi, let's download a photo of [my Dad giving two thumbs up to a big pizza from the Learn PyTorch for Deep Learning GitHub](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg).\n",
        "\n",
        "We download the image using Python's `requests` module.\n",
        "\n",
        "> **Note:** If you're using Google Colab, you can also upload an image to the current session by going to the left hand side menu -> Files -> Upload to session storage. Beware though, this image will delete when your Google Colab session ends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkfA_3r474l7",
        "outputId": "c75aa2b9-4c6a-4f8a-c08b-7c84686c5154"
      },
      "outputs": [],
      "source": [
        "# Download custom image\n",
        "import requests\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "    with open(custom_image_path, \"wb\") as f:\n",
        "        # When downloading from GitHub, need to use the \"raw\" file link\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
        "        print(f\"Downloading {custom_image_path}...\")\n",
        "        f.write(request.content)\n",
        "else:\n",
        "    print(f\"{custom_image_path} already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIOBPXAh74l7"
      },
      "source": [
        "### 9.1 Loading in a custom image with PyTorch\n",
        "\n",
        "Excellent! Looks like we've got a custom image downloaded and ready to go at `data/04-pizza-dad.jpeg`. Time to load it in. PyTorch's `torchvision` has several input and output (\"IO\" or \"io\" for short) methods for reading and writing images and video in [`torchvision.io`](https://pytorch.org/vision/stable/io.html). Since we want to load in an image, we'll use [`torchvision.io.read_image()`](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image). This method will read a JPEG or PNG image and turn it into a 3 dimensional RGB or grayscale `torch.Tensor` with values of datatype `uint8` in range `[0, 255]`.\n",
        "\n",
        "Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g0G1fIf74l7",
        "outputId": "5587004a-bc5f-4afc-c47b-b31408dd09b2"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "# Read in custom image\n",
        "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwL3eQu374l8"
      },
      "source": [
        "Nice! Looks like our image is in tensor format, however, is this image format compatible with our model? Our `custom_image` tensor is of datatype `torch.uint8` and its values are between `[0, 255]`. But our model takes image tensors of datatype `torch.float32` and with values between `[0, 1]`. So before we use our custom image with our model, **we'll need to convert it to the same format as the data our model is trained on**. If we don't do this, our model will error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMh_2UQK74l8"
      },
      "source": [
        "Let's convert our custom image to the same datatype as what our model was trained on (`torch.float32`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1-ITboX74l8",
        "outputId": "20ea46fb-e470-4e49-f9f9-aa91892998e6"
      },
      "outputs": [],
      "source": [
        "# Load in custom image and convert the tensor values to float32\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "\n",
        "# Divide the image pixel values by 255 to get them between [0, 1]\n",
        "custom_image = custom_image / 255.\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
        "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDDF9bSv74l8"
      },
      "source": [
        "### 9.2 Predicting on custom images with a trained PyTorch model\n",
        "\n",
        "Beautiful, it looks like our image data is now in the same format our model was trained on.\n",
        "\n",
        "Except for one thing...\n",
        "\n",
        "It's `shape`.\n",
        "\n",
        "Our model was trained on images with shape `[3, 64, 64]`, whereas our custom image is currently `[3, 4032, 3024]`.\n",
        "\n",
        "How could we make sure our custom image is the same shape as the images our model was trained on?\n",
        "\n",
        "Are there any `torchvision.transforms` that could help?\n",
        "\n",
        "Before we answer that question, let's plot the image with `matplotlib` to make sure it looks okay, remember we'll have to permute the dimensions from `CHW` to `HWC` to suit `matplotlib`'s requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "PfmfDglU74l8",
        "outputId": "bda3c80e-dfd5-4354-a69a-7952640a21bb"
      },
      "outputs": [],
      "source": [
        "# Plot custom image\n",
        "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
        "plt.title(f\"Image shape: {custom_image.shape}\")\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXD0uRC074l8"
      },
      "source": [
        "Two thumbs up! Now how could we get our image to be the same size as the images our model was trained on? One way to do so is with `torchvision.transforms.Resize()`. Let's compose a transform pipeline to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBR6X-of74l8",
        "outputId": "69034267-2a76-42f6-c1e0-62636deb1123"
      },
      "outputs": [],
      "source": [
        "# Create transform pipleine to resize image\n",
        "custom_image_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "])\n",
        "\n",
        "# Transform target image\n",
        "custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "# Print out original shape and new shape\n",
        "print(f\"Original shape: {custom_image.shape}\")\n",
        "print(f\"New shape: {custom_image_transformed.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f03AOnNn74l8"
      },
      "source": [
        "Let's finally make a prediction on our own custom image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is6FpMMB74l8",
        "outputId": "7f5b43eb-8d30-47ed-9c66-5ab14c2dbf35"
      },
      "outputs": [],
      "source": [
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "    # Add an extra dimension to image\n",
        "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
        "\n",
        "    # Print out different shapes\n",
        "    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
        "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
        "\n",
        "    # Make a prediction on image with an extra dimension\n",
        "    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U51xWfZ74l8"
      },
      "source": [
        "Yes!!! It looks like it worked!\n",
        "\n",
        "Now let's take a look at our model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzV73Wzg74l8",
        "outputId": "ef233963-5009-40c8-c8f3-3b38bf173faa"
      },
      "outputs": [],
      "source": [
        "custom_image_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylytFl4r74l8"
      },
      "source": [
        "Alright, these are still in *logit form* (the raw outputs of a model are called logits).\n",
        "\n",
        "Let's convert them from logits -> prediction probabilities -> prediction labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q88ekFFn74l8",
        "outputId": "36b0696d-fc64-42d1-cc48-80fc5568106b"
      },
      "outputs": [],
      "source": [
        "# Print out prediction logits\n",
        "print(f\"Prediction logits: {custom_image_pred}\")\n",
        "\n",
        "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
        "\n",
        "# Convert prediction probabilities -> prediction labels\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
        "print(f\"Prediction label: {custom_image_pred_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmvkgXPr74l8"
      },
      "source": [
        "Alright!\n",
        "\n",
        "Looking good.\n",
        "\n",
        "But of course our prediction label is still in index/tensor form.\n",
        "\n",
        "We can convert it to a string class name prediction by indexing on the `class_names` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "caX7bKPa74l8",
        "outputId": "c9bb470b-e67f-495b-ee7b-c83b2d8d3c5c"
      },
      "outputs": [],
      "source": [
        "# Find the predicted label\n",
        "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
        "custom_image_pred_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUImUqW274l8"
      },
      "source": [
        "Oops! It looks like the model gets the prediction wrong, but we expected it from the performances obtained above!\n",
        "\n",
        "> **Note:** The model in its current form will predict \"pizza\", \"steak\" or \"sushi\" no matter what image it's given. If you wanted your model to predict on a different class, you'd have to train it to do so.\n",
        "\n",
        "But if we check the `custom_image_pred_probs`, we'll notice that the model gives almost equal weight (the values are similar) to every class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4niwn20074l8",
        "outputId": "2e33f4b9-cc70-44ca-b261-b89b82209964"
      },
      "outputs": [],
      "source": [
        "# The values of the prediction probabilities are quite similar\n",
        "custom_image_pred_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpZsrH9c74l9"
      },
      "source": [
        "Having prediction probabilities this similar could mean a couple of things:\n",
        "1. The model is trying to predict all three classes at the same time (there may be an image containing pizza, steak and sushi).\n",
        "2. The model doesn't really know what it wants to predict and is in turn just assigning similar values to each of the classes.\n",
        "\n",
        "Our case is number 2, since our model is poorly trained, it is basically *guessing* the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfK9XAR874l9"
      },
      "source": [
        "### 11.3 Putting custom image prediction together: building a function\n",
        "\n",
        "Doing all of the above steps every time you'd like to make a prediction on a custom image would quickly become tedious.\n",
        "\n",
        "So let's put them all together in a function we can easily use over and over again.\n",
        "\n",
        "Specifically, let's make a function that:\n",
        "1. Takes in a target image path and converts to the right datatype for our model (`torch.float32`).\n",
        "2. Makes sure the target image pixel values are in the range `[0, 1]`.\n",
        "3. Transforms the target image if necessary.\n",
        "4. Makes sure the model is on the target device.\n",
        "5. Makes a prediction on the target image with a trained model (ensuring the image is the right size and on the same device as the model).\n",
        "6. Converts the model's output logits to prediction probabilities.\n",
        "7. Converts the prediction probabilities to prediction labels.\n",
        "8. Plots the target image alongside the model prediction and prediction probability.\n",
        "\n",
        "A fair few steps but we've got this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8EWXfNC74l9"
      },
      "outputs": [],
      "source": [
        "def pred_and_plot_image(model: torch.nn.Module,\n",
        "                        image_path: str,\n",
        "                        class_names: List[str] = None,\n",
        "                        transform=None,\n",
        "                        device: torch.device = device):\n",
        "    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n",
        "\n",
        "    # 1. Load in image and convert the tensor values to float32\n",
        "    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
        "\n",
        "    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n",
        "    target_image = target_image / 255.\n",
        "\n",
        "    # 3. Transform if necessary\n",
        "    if transform:\n",
        "        target_image = transform(target_image)\n",
        "\n",
        "    # 4. Make sure the model is on the target device\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. Turn on model evaluation mode and inference mode\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Add an extra dimension to the image\n",
        "        target_image = target_image.unsqueeze(dim=0)\n",
        "\n",
        "        # Make a prediction on image with an extra dimension and send it to the target device\n",
        "        target_image_pred = model(target_image.to(device))\n",
        "\n",
        "    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "    # 7. Convert prediction probabilities -> prediction labels\n",
        "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "    # 8. Plot the image alongside the prediction and prediction probability\n",
        "    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n",
        "    if class_names:\n",
        "        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "    else:\n",
        "        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "    plt.title(title)\n",
        "    plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81vUTE6v74l9"
      },
      "source": [
        "What a nice looking function, let's test it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "UD_DrJ7-74l9",
        "outputId": "7a3b3b20-92c0-4a65-ae74-bdaf37916482"
      },
      "outputs": [],
      "source": [
        "# Pred on our custom image\n",
        "pred_and_plot_image(model=model_1,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names,\n",
        "                    transform=custom_image_transform,\n",
        "                    device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBpOUqim74l9"
      },
      "source": [
        "Two thumbs up again! We did it! The image is pixelated too because we resized it to `[64, 64]` using `custom_image_transform`.\n",
        "\n",
        "Our model needs some improvements for sure, but we have a good code baseline to make it work.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "interpreter": {
      "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "007710231a5449ff964e5f97db350d9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff6fe61a1a74ca5a499cf7217a4bfed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bca8cd9332342bca58ad5166b7846d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea57a3613d3d40e09344b97cf2d86150",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ff6fe61a1a74ca5a499cf7217a4bfed",
            "value": 5
          }
        },
        "28e1956beb814a10b3cb34dae5c87f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81585de2060445628b6b2f1ad23b311d",
            "placeholder": "",
            "style": "IPY_MODEL_5c82c78179b54a5d8b860e3e0e6cd9cf",
            "value": "5/5[00:05&lt;00:00,1.16s/it]"
          }
        },
        "2bd81d26df9943db88d802b4d8d90ad9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36166176a93444debf0869de3998c5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dc74616c7884742a6065b2124e2ca53",
            "placeholder": "",
            "style": "IPY_MODEL_ebf44251016840d9a8cad51dff0c9900",
            "value": "5/5[00:05&lt;00:00,1.11s/it]"
          }
        },
        "39bf1e9ad56048f7ac25477f0331b5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dc74616c7884742a6065b2124e2ca53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fda915bb2d640e082c5089ba0469a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_762479bb744547a39cf67136f286f78a",
            "placeholder": "",
            "style": "IPY_MODEL_bb7f7d862a7e4fd49a1c0b0fd6138ca4",
            "value": "100%"
          }
        },
        "42ef049d73e64ea58e73773060710c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7491de13f92e4d5eafaa2d0b962475ef",
              "IPY_MODEL_1bca8cd9332342bca58ad5166b7846d2",
              "IPY_MODEL_36166176a93444debf0869de3998c5b8"
            ],
            "layout": "IPY_MODEL_39bf1e9ad56048f7ac25477f0331b5cb"
          }
        },
        "4e4165a0ef324ebaa95195c924dd7ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c82c78179b54a5d8b860e3e0e6cd9cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66e92651313f4afa9d30f2a1ebd09f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_007710231a5449ff964e5f97db350d9c",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78f78732a62849ffad723b59e5fb2d29",
            "value": 5
          }
        },
        "7491de13f92e4d5eafaa2d0b962475ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd81d26df9943db88d802b4d8d90ad9",
            "placeholder": "",
            "style": "IPY_MODEL_4e4165a0ef324ebaa95195c924dd7ef4",
            "value": "100%"
          }
        },
        "762479bb744547a39cf67136f286f78a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78f78732a62849ffad723b59e5fb2d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81585de2060445628b6b2f1ad23b311d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba394d6b117a4b0aacc3e15990fefe8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3fda915bb2d640e082c5089ba0469a5e",
              "IPY_MODEL_66e92651313f4afa9d30f2a1ebd09f20",
              "IPY_MODEL_28e1956beb814a10b3cb34dae5c87f9e"
            ],
            "layout": "IPY_MODEL_cff909918cc54513820f0a8a24dd57cd"
          }
        },
        "bb7f7d862a7e4fd49a1c0b0fd6138ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cff909918cc54513820f0a8a24dd57cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea57a3613d3d40e09344b97cf2d86150": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebf44251016840d9a8cad51dff0c9900": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
